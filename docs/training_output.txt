-------------------------------------------------------------------------------------------
-------------------------------------------------------------------------------------------
V1.0: batch_size=512, epochs=50 训练时间4个小时
-------------------------------------------------------------------------------------------
-------------------------------------------------------------------------------------------
Epoch 1/50
2018-06-06 23:55:47.736378: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
104092/104092 [==============================] - 293s 3ms/step - loss: 1.3047 - acc: 0.5096 - val_loss: 1.2900 - val_acc: 0.5079
Epoch 2/50
104092/104092 [==============================] - 292s 3ms/step - loss: 1.2740 - acc: 0.5110 - val_loss: 1.2424 - val_acc: 0.5079
Epoch 3/50
104092/104092 [==============================] - 292s 3ms/step - loss: 1.2445 - acc: 0.5110 - val_loss: 1.2394 - val_acc: 0.5079
Epoch 4/50
104092/104092 [==============================] - 292s 3ms/step - loss: 1.2368 - acc: 0.5109 - val_loss: 1.2320 - val_acc: 0.5080
Epoch 5/50
104092/104092 [==============================] - 292s 3ms/step - loss: 1.2358 - acc: 0.5113 - val_loss: 1.2333 - val_acc: 0.5081
Epoch 6/50
104092/104092 [==============================] - 292s 3ms/step - loss: 1.2349 - acc: 0.5119 - val_loss: 1.2325 - val_acc: 0.5084
Epoch 7/50
104092/104092 [==============================] - 293s 3ms/step - loss: 1.2340 - acc: 0.5120 - val_loss: 1.2274 - val_acc: 0.5092
Epoch 8/50
104092/104092 [==============================] - 293s 3ms/step - loss: 1.2292 - acc: 0.5133 - val_loss: 1.2289 - val_acc: 0.5083
Epoch 9/50
104092/104092 [==============================] - 293s 3ms/step - loss: 1.2272 - acc: 0.5136 - val_loss: 1.2258 - val_acc: 0.5090
Epoch 10/50
104092/104092 [==============================] - 292s 3ms/step - loss: 1.2547 - acc: 0.5108 - val_loss: 1.2396 - val_acc: 0.5079

Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.
Epoch 11/50
104092/104092 [==============================] - 292s 3ms/step - loss: 1.2347 - acc: 0.5107 - val_loss: 1.2290 - val_acc: 0.5086
Epoch 12/50
104092/104092 [==============================] - 293s 3ms/step - loss: 1.2266 - acc: 0.5114 - val_loss: 1.2291 - val_acc: 0.5082
Epoch 13/50
104092/104092 [==============================] - 293s 3ms/step - loss: 1.2197 - acc: 0.5122 - val_loss: 1.2167 - val_acc: 0.5094
Epoch 14/50
104092/104092 [==============================] - 293s 3ms/step - loss: 1.2115 - acc: 0.5144 - val_loss: 1.2036 - val_acc: 0.5132
Epoch 15/50
104092/104092 [==============================] - 292s 3ms/step - loss: 1.2050 - acc: 0.5156 - val_loss: 1.1996 - val_acc: 0.5145
Epoch 16/50
104092/104092 [==============================] - 292s 3ms/step - loss: 1.2015 - acc: 0.5169 - val_loss: 1.1950 - val_acc: 0.5166

...

Epoch 41/50
104092/104092 [==============================] - 292s 3ms/step - loss: 1.0754 - acc: 0.5662 - val_loss: 1.0685 - val_acc: 0.5682
Epoch 42/50
104092/104092 [==============================] - 292s 3ms/step - loss: 1.0717 - acc: 0.5682 - val_loss: 1.0705 - val_acc: 0.5679
Epoch 43/50
104092/104092 [==============================] - 292s 3ms/step - loss: 1.0677 - acc: 0.5689 - val_loss: 1.0697 - val_acc: 0.5681
Epoch 44/50
104092/104092 [==============================] - 292s 3ms/step - loss: 1.0637 - acc: 0.5702 - val_loss: 1.0628 - val_acc: 0.5707
Epoch 45/50
104092/104092 [==============================] - 292s 3ms/step - loss: 1.0608 - acc: 0.5723 - val_loss: 1.0703 - val_acc: 0.5667
Epoch 46/50
104092/104092 [==============================] - 292s 3ms/step - loss: 1.0586 - acc: 0.5727 - val_loss: 1.0594 - val_acc: 0.5729
Epoch 47/50
104092/104092 [==============================] - 292s 3ms/step - loss: 1.0566 - acc: 0.5734 - val_loss: 1.0664 - val_acc: 0.5688
Epoch 48/50
104092/104092 [==============================] - 292s 3ms/step - loss: 1.0558 - acc: 0.5729 - val_loss: 1.0618 - val_acc: 0.5690
Epoch 49/50
104092/104092 [==============================] - 292s 3ms/step - loss: 1.0519 - acc: 0.5751 - val_loss: 1.0559 - val_acc: 0.5721

Epoch 00049: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.
Epoch 50/50
104092/104092 [==============================] - 292s 3ms/step - loss: 1.0407 - acc: 0.5794 - val_loss: 1.0468 - val_acc: 0.5763

-------------------------------------------------------------------------------------------
-------------------------------------------------------------------------------------------
V2.0: batch_size=1024, epochs=300, 训练时间22小时，学习速度太慢
-------------------------------------------------------------------------------------------
-------------------------------------------------------------------------------------------
Epoch 1/300
104092/104092 [==============================] - 265s 3ms/step - loss: 1.3147 - acc: 0.5075 - val_loss: 1.2838 - val_acc: 0.5079
Epoch 2/300
104092/104092 [==============================] - 265s 3ms/step - loss: 1.2849 - acc: 0.5110 - val_loss: 1.2800 - val_acc: 0.5079
Epoch 3/300
104092/104092 [==============================] - 265s 3ms/step - loss: 1.2774 - acc: 0.5110 - val_loss: 1.2655 - val_acc: 0.5079
Epoch 4/300
104092/104092 [==============================] - 266s 3ms/step - loss: 1.2521 - acc: 0.5110 - val_loss: 1.2470 - val_acc: 0.5079
Epoch 5/300
104092/104092 [==============================] - 265s 3ms/step - loss: 1.2473 - acc: 0.5110 - val_loss: 1.2391 - val_acc: 0.5079
...
Epoch 286/300
104092/104092 [==============================] - 267s 3ms/step - loss: 1.1139 - acc: 0.5513 - val_loss: 1.1087 - val_acc: 0.5531
Epoch 287/300
104092/104092 [==============================] - 267s 3ms/step - loss: 1.1127 - acc: 0.5520 - val_loss: 1.1094 - val_acc: 0.5527
Epoch 288/300
104092/104092 [==============================] - 268s 3ms/step - loss: 1.1135 - acc: 0.5523 - val_loss: 1.1089 - val_acc: 0.5530
Epoch 289/300
104092/104092 [==============================] - 267s 3ms/step - loss: 1.1129 - acc: 0.5521 - val_loss: 1.1096 - val_acc: 0.5527
Epoch 290/300
104092/104092 [==============================] - 267s 3ms/step - loss: 1.1129 - acc: 0.5515 - val_loss: 1.1089 - val_acc: 0.5535
Epoch 291/300
104092/104092 [==============================] - 266s 3ms/step - loss: 1.1131 - acc: 0.5524 - val_loss: 1.1087 - val_acc: 0.5536
Epoch 292/300
104092/104092 [==============================] - 266s 3ms/step - loss: 1.1130 - acc: 0.5516 - val_loss: 1.1086 - val_acc: 0.5530
Epoch 293/300
104092/104092 [==============================] - 267s 3ms/step - loss: 1.1124 - acc: 0.5529 - val_loss: 1.1087 - val_acc: 0.5522
Epoch 294/300
104092/104092 [==============================] - 267s 3ms/step - loss: 1.1133 - acc: 0.5519 - val_loss: 1.1093 - val_acc: 0.5526
Epoch 295/300
104092/104092 [==============================] - 267s 3ms/step - loss: 1.1133 - acc: 0.5507 - val_loss: 1.1088 - val_acc: 0.5528
